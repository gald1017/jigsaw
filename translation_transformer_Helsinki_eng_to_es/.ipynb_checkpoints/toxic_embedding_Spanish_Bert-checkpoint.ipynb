{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "toxic-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "#from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "worse-volume",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok(tokenizer, text):\n",
    "    return tokenizer.encode_plus(text, \n",
    "                                    add_special_tokens=True,\n",
    "                                    max_length=510,\n",
    "                                    padding='longest', \n",
    "                                    truncation=True,\n",
    "                                      return_token_type_ids=True,\n",
    "                                      return_attention_mask=True,\n",
    "                                      return_tensors='pt'\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unexpected-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_embed(df, model, tokenizer):\n",
    "    l = []\n",
    "    for i, review in enumerate(df.review_body):\n",
    "        tokened = tok(tokenizer, review)\n",
    "        #print(model(**tokenizer))\n",
    "        l.append(embed(model_mul, tokened).numpy())\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "referenced-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Tuple\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import torch\n",
    "\n",
    "def embed(model, tokens_tensor ):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tokens_tensor = tokens_tensor.to('cuda')\n",
    "        model.to('cuda')\n",
    "        outputs = model(**tokens_tensor)\n",
    "\n",
    "        # Evaluating the model will return a different number of objects based on \n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "        # becase we set `output_hidden_states = True`, the third item will be the \n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "        hidden_states = outputs[2]\n",
    "        token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        # Stores the token vectors, with shape [6 x 768]\n",
    "    \n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # `token_embeddings` is a [6 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [6 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "        \n",
    "    token_vecs = hidden_states[-2][0]\n",
    "\n",
    "    # Calculate the average of all 6 token vectors.\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    \n",
    "    return sentence_embedding\n",
    "\n",
    "def embed_cls(model, tokens_tensor):\n",
    "    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tokens_tensor = tokens_tensor.to('cuda')       \n",
    "        model.to('cuda')\n",
    "        \n",
    "        #if torch.cuda.device_count() > 1:\n",
    "            #print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            #model = nn.DataParallel(model)\n",
    "        #else:\n",
    "            #model.to(device)\n",
    "        \n",
    "        outputs = model(**tokens_tensor)\n",
    "        \n",
    "        return outputs.pooler_output\n",
    "        \n",
    "class BertTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "            self,\n",
    "            bert_tokenizer,\n",
    "            bert_model,\n",
    "            max_length: int = 510,\n",
    "            embedding_func = None,\n",
    "    ):\n",
    "        self.tokenizer = bert_tokenizer\n",
    "        self.model = bert_model\n",
    "        self.model.eval()\n",
    "        self.max_length = max_length\n",
    "        self.embedding_func = embedding_func\n",
    "\n",
    "        if self.embedding_func is None:\n",
    "            self.embedding_func = lambda x: x[0][:, 0, :].squeeze()\n",
    "\n",
    "    def _tokenize(self, text: str) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        # Tokenize the text with the provided tokenizer\n",
    "#         tokenized_text = self.tokenizer.encode_plus(text,\n",
    "#                                                     add_special_tokens=True,\n",
    "#                                                     max_length=self.max_length\n",
    "#                                                     )[\"input_ids\"]\n",
    "        \n",
    "        tokenized_text = self.tokenizer.encode_plus(text, \n",
    "                                    add_special_tokens=True,\n",
    "                                    max_length=self.max_length,\n",
    "                                    padding='longest', \n",
    "                                    truncation=True,\n",
    "                                    return_token_type_ids=True,\n",
    "                                    return_attention_mask=True,\n",
    "                                    return_tensors='pt'\n",
    "                                    )\n",
    "        return tokenized_text\n",
    "\n",
    "\n",
    "    def _tokenize_and_predict(self, text: str) -> torch.tensor:\n",
    "        tokenized = self._tokenize(text)\n",
    "\n",
    "        #embeddings = self.model(**tokenized)\n",
    "        return self.embedding_func(self.model, tokenized)\n",
    "\n",
    "    def transform(self, text: List[str]):\n",
    "        if isinstance(text, pd.Series):\n",
    "            text = text.tolist()\n",
    "        \n",
    "#         return torch.stack([self._tokenize_and_predict(string) for string in text]).cpu()\n",
    "\n",
    "        return torch.stack([self._tokenize_and_predict(text)]).cpu()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"No fitting necessary so we just return ourselves\"\"\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-situation",
   "metadata": {},
   "source": [
    "# Data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-discharge",
   "metadata": {},
   "source": [
    "### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fuzzy-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'data/jigsaw-toxic-comment-train.csv'\n",
    "\n",
    "eng_ds = pd.read_csv(FILE_PATH)\n",
    "eng_text = eng_ds.comment_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-hungarian",
   "metadata": {},
   "source": [
    "### Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fallen-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'data/val_es_text_only.csv'\n",
    "\n",
    "es_ds = pd.read_csv(FILE_PATH)\n",
    "es_text = es_ds.comment_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "finite-taylor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>input_word_ids</th>\n",
       "      <th>input_mask</th>\n",
       "      <th>all_segment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Doctor Who adlı viki başlığına 12. doctor olar...</td>\n",
       "      <td>(101, 17376, 14516, 19165, 56324, 10116, 24542...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Вполне возможно, но я пока не вижу необходимо...</td>\n",
       "      <td>(101, 511, 53204, 36689, 44504, 117, 11279, 57...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Quindi tu sei uno di quelli   conservativi  , ...</td>\n",
       "      <td>(101, 35921, 17938, 13055, 13868, 11381, 10120...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Malesef gerçekleştirilmedi ancak şöyle bir şey...</td>\n",
       "      <td>(101, 59170, 16822, 99087, 10284, 83972, 51782...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>:Resim:Seldabagcan.jpg resminde kaynak sorunu ...</td>\n",
       "      <td>(101, 131, 32070, 11759, 131, 11045, 23388, 10...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63807</th>\n",
       "      <td>63807</td>\n",
       "      <td>No, non risponderò, come preannunciato. Prefer...</td>\n",
       "      <td>(101, 10657, 117, 10446, 29956, 54609, 102754,...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63808</th>\n",
       "      <td>63808</td>\n",
       "      <td>Ciao, I tecnici della Wikimedia Foundation sta...</td>\n",
       "      <td>(101, 51457, 14875, 117, 146, 10361, 101788, 1...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63809</th>\n",
       "      <td>63809</td>\n",
       "      <td>innnazitutto ti ringrazio per i ringraziamenti...</td>\n",
       "      <td>(101, 15203, 10219, 46680, 109056, 14382, 2155...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63810</th>\n",
       "      <td>63810</td>\n",
       "      <td>Kaç olumlu oy gerekiyor? Şu an 7 oldu.  Hayır...</td>\n",
       "      <td>(101, 25444, 13406, 30668, 107357, 183, 10157,...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63811</th>\n",
       "      <td>63811</td>\n",
       "      <td>Te pido disculpas. La verdad es que no me per...</td>\n",
       "      <td>(101, 21452, 24109, 10317, 110076, 70285, 1010...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63812 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                       comment_text  \\\n",
       "0          0  Doctor Who adlı viki başlığına 12. doctor olar...   \n",
       "1          1   Вполне возможно, но я пока не вижу необходимо...   \n",
       "2          2  Quindi tu sei uno di quelli   conservativi  , ...   \n",
       "3          3  Malesef gerçekleştirilmedi ancak şöyle bir şey...   \n",
       "4          4  :Resim:Seldabagcan.jpg resminde kaynak sorunu ...   \n",
       "...      ...                                                ...   \n",
       "63807  63807  No, non risponderò, come preannunciato. Prefer...   \n",
       "63808  63808  Ciao, I tecnici della Wikimedia Foundation sta...   \n",
       "63809  63809  innnazitutto ti ringrazio per i ringraziamenti...   \n",
       "63810  63810   Kaç olumlu oy gerekiyor? Şu an 7 oldu.  Hayır...   \n",
       "63811  63811   Te pido disculpas. La verdad es que no me per...   \n",
       "\n",
       "                                          input_word_ids  \\\n",
       "0      (101, 17376, 14516, 19165, 56324, 10116, 24542...   \n",
       "1      (101, 511, 53204, 36689, 44504, 117, 11279, 57...   \n",
       "2      (101, 35921, 17938, 13055, 13868, 11381, 10120...   \n",
       "3      (101, 59170, 16822, 99087, 10284, 83972, 51782...   \n",
       "4      (101, 131, 32070, 11759, 131, 11045, 23388, 10...   \n",
       "...                                                  ...   \n",
       "63807  (101, 10657, 117, 10446, 29956, 54609, 102754,...   \n",
       "63808  (101, 51457, 14875, 117, 146, 10361, 101788, 1...   \n",
       "63809  (101, 15203, 10219, 46680, 109056, 14382, 2155...   \n",
       "63810  (101, 25444, 13406, 30668, 107357, 183, 10157,...   \n",
       "63811  (101, 21452, 24109, 10317, 110076, 70285, 1010...   \n",
       "\n",
       "                                              input_mask  \\\n",
       "0      (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2      (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3      (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4      (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                  ...   \n",
       "63807  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "63808  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "63809  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "63810  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "63811  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                          all_segment_id  \n",
       "0      (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1      (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2      (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4      (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                  ...  \n",
       "63807  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "63808  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "63809  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "63810  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "63811  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[63812 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILE_PATH = 'data/test-processed-seqlen128.csv'\n",
    "pd.read_csv(FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-liberty",
   "metadata": {},
   "source": [
    "### Spanish ds translated to Englisgh - AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "reduced-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'translation/val_es_text_only__en_AWS_TRANSLATED.csv'\n",
    "\n",
    "es_t_eng_aws_ds = pd.read_csv(FILE_PATH)\n",
    "es_t_eng_aws_text = es_t_eng_aws_ds.TranslatedText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-australia",
   "metadata": {},
   "source": [
    "# Labse BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "descending-bridges",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e9dd6da9104c7d87e9bf40185aa61a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1daff6bab1864987996d318fe7f332d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f4c53ae1a944489650c025589f3b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/62.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36736005241641ebbc5973e91e81bfb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/472 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5ede1b8b114f2ead945f23217889cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_mul = BertTokenizer.from_pretrained(\"pvl/labse_bert\")\n",
    "model_mul = BertModel.from_pretrained('pvl/labse_bert',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "severe-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert_transformer = BertTransformer(tokenizer_mul, model_mul, embedding_func=embed)\n",
    "bert_transformer = BertTransformer(tokenizer_mul, model_mul, embedding_func=embed_cls)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "prescription-arthur",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101.24582242965698\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import itertools\n",
    "\n",
    "texts = [es_t_eng_aws_text]\n",
    "files_name = ['es_t_eng_aws_text.pkl']\n",
    "\n",
    "\n",
    "for i,t in enumerate(texts):\n",
    "    emebedded = []\n",
    "    start = time.time()\n",
    "    for com in t:\n",
    "        emebedded.append(bert_transformer.transform(com))\n",
    "    \n",
    "    with open(files_name[i], 'wb') as f:\n",
    "        torch.save(list(itertools.chain(*emebedded)), f)    \n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-district",
   "metadata": {},
   "source": [
    "# Multilingual BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ecological-substance",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nTFBertModel requires the TensorFlow library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://www.tensorflow.org/install and follow the ones that match your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-973e621e308f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer_mul\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-multilingual-cased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_mul\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-multilingual-cased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages/transformers/utils/dummy_tf_objects.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mrequires_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mrequires_tf\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__name__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTENSORFLOW_IMPORT_ERROR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: \nTFBertModel requires the TensorFlow library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://www.tensorflow.org/install and follow the ones that match your environment.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "tokenizer_mul = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model_mul = TFBertModel.from_pretrained(\"bert-base-multilingual-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "flush-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_transformer = BertTransformer(tokenizer_mul, model_mul, embedding_func=embed_cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-adaptation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import itertools\n",
    "\n",
    "texts = [es_text, eng_text]\n",
    "files_name = ['validation_es_only_text_CLS_multilingual_bert.pkl', 'jigsaw_toxic-comment_train_embedded_CLS_multilingual_bert.pkl']\n",
    "\n",
    "\n",
    "for i,t in enumerate(texts):\n",
    "    emebedded = []\n",
    "    start = time.time()\n",
    "    for com in t:\n",
    "        emebedded.append(bert_transformer.transform(com))\n",
    "    \n",
    "    with open(files_name[i], 'wb') as f:\n",
    "        torch.save(list(itertools.chain(*emebedded)), f)    \n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-magazine",
   "metadata": {},
   "source": [
    "# xlm-roberta-large-xnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "willing-identity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.5.1\n",
      "Uninstalling transformers-4.5.1:\n",
      "  Successfully uninstalled transformers-4.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall transformers -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sublime-jordan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 18.0 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.95\n"
     ]
    }
   ],
   "source": [
    "!pip install  sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "owned-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import XLMRobertaModel , XLMRobertaTokenizer\n",
    "\n",
    "tokenizer_mul = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "model_mul = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\", output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "promotional-ocean",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "advised-receptor",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_transformer = BertTransformer(tokenizer_mul, model_mul, embedding_func=embed_cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "economic-visibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4355.596848249435\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import itertools\n",
    "\n",
    "texts = [es_text, eng_text]\n",
    "files_name = ['validation_es_only_text_CLS_xlm-roberta-large-xnli.pkl', 'jigsaw_toxic-comment_train_embedded_CLS_xlm-roberta-large-xnli.pkl']\n",
    "\n",
    "texts = [eng_text]\n",
    "files_name = ['jigsaw_toxic-comment_train_embedded_CLS_xlm-roberta-large-xnli_2.pkl']\n",
    "\n",
    "\n",
    "\n",
    "for i,t in enumerate(texts):\n",
    "    emebedded = []\n",
    "    start = time.time()\n",
    "    for com in t:\n",
    "        emebedded.append(bert_transformer.transform(com))\n",
    "    \n",
    "    with open(files_name[i], 'wb') as f:\n",
    "        torch.save(list(itertools.chain(*emebedded)), f)    \n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-cookbook",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p36",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
